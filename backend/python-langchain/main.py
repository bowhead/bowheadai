from functools import wraps
from flask import Flask, request, session, jsonify
from flask_cors import CORS
import os
from os import getenv
import requests
import shutil
from flask_socketio import SocketIO, emit, disconnect
from flask_session import Session
from flask_login import login_user, current_user, LoginManager, login_required
from os import getenv
from dotenv import load_dotenv
from pathvalidate import sanitize_filename

from pypdf import PdfReader
import os
from PIL import Image
import pytesseract
from dotenv import load_dotenv
import re

from models.User import User
from langchain.document_loaders import DirectoryLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.llms import OpenAI
from langchain.text_splitter import CharacterTextSplitter
from langchain.agents.tools import Tool
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser
from langchain.prompts import StringPromptTemplate
from langchain import LLMChain
from typing import List, Union
from langchain.schema import AgentAction, AgentFinish, OutputParserException
from langchain.prompts import PromptTemplate
from src.pupmed import PubMedRetriever
from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory
from langchain.callbacks.base import BaseCallbackHandler
from langchain.embeddings import HuggingFaceInstructEmbeddings


from queue import Queue
import sys
from flask import Response, stream_with_context
import threading 

sys.setrecursionlimit(1000)
# Load environment variables from .env file
load_dotenv()

# Access the URL variables
api_url = getenv('LANGCHAIN_UPLOAD_ENDPOINT')
delete_url = getenv('LANGCHAIN_DELETE_ENDPOINT')
cors_domains = getenv('CORS_DOMAINS').split(',')
cookie_domain = getenv('COOKIE_DOMAIN')
app = Flask(__name__)
app.secret_key = getenv('SECRET_KEY', '')
app.config['SESSION_TYPE'] = 'filesystem'
app.config['SESSION_COOKIE_SAMESITE'] = 'None'
app.config['SESSION_COOKIE_SECURE'] = True
app.config['SESSION_COOKIE_DOMAIN'] = cookie_domain
app.config['REMEMBER_COOKIE_SAMESITE'] = 'None'
app.config['REMEMBER_COOKIE_SECURE'] = True
app.config['REMEMBER_COOKIE_DOMAIN'] = cookie_domain

CORS(app, supports_credentials=True, origins=cors_domains)
login_manager = LoginManager()
login_manager.init_app(app)
login_manager.login_view = "login"
Session(app)

socketio = SocketIO(app, cors_allowed_origins=cors_domains, manage_session=False)

# *** TEMPLATE ***
template = """Answer the following questions as best you can only using the tools, You are a friendly, conversational personal medical assistant. 
You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times until there is enough information to answer the Question)
Thought: I now know the final answer
Final Answer: the final answer to the original input question. The answer should be a text with the "answer" following by a json with the key "sources" as a list of urls generated by the uid. Add "END" at the end of the final answer.

Chat history:
{chat_history}

New question: {input}
{agent_scratchpad}"""

prompt_template_health_vector = """Use the following context about medical records to show to the user the information they need, help find exactly what they want. 
    You must give information about user data, especially negative results that may affect the user's health. 

    The context could be in multiple languages. You should always respond in English even if the context is in another language.
    It's ok if you don't know the answer. 

    Context:
    {context}

    Question: {question}
    Answer:"""

prompt_template_trials_vector = """Use the following context about medical trials to match the users health data (inclusion and exclusion criteria) with the best medical trial option. 

    If the trial is for a specific sex or age, you need to be sure you have this data of the patient, otherwise it's not a match.
    It's ok if you don't find a match.
     
    
    Always refer by the nctID when you talk about a trial in the answer.

    Context:
    {context}

    Question: {question}
    Answer:"""
q = Queue()
# *** Set up a prompt template ***
class CustomPromptTemplate(StringPromptTemplate):
    # The template to use
    template: str
    # The list of tools available
    tools: List[Tool]

    def format(self, **kwargs) -> str:
        # Get the intermediate steps (AgentAction, Observation tuples)
        # Format them in a particular way
        intermediate_steps = kwargs.pop("intermediate_steps")
        thoughts = ""
        for action, observation in intermediate_steps:
            thoughts += action.log
            thoughts += f"\nObservation: {observation}\nThought: "
        # Set the agent_scratchpad variable to that value
        kwargs["agent_scratchpad"] = thoughts
        # Create a tools variable from the list of tools provided
        kwargs["tools"] = "\n".join([f"{tool.name}: {tool.description}" for tool in self.tools])
        # Create a list of tool names for the tools provided
        kwargs["tool_names"] = ", ".join([tool.name for tool in self.tools])
        return self.template.format(**kwargs)

# *** Output Parser ***
class CustomOutputParser(AgentOutputParser):
    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:
        # Check if agent should finish
        if "Final Answer:" in llm_output:
            return AgentFinish(
                # Return values is generally always a dictionary with a single `output` key
                # It is not recommended to try anything else at the moment :)
                return_values={"output": llm_output.split("Final Answer:")[-1].strip()},
                log=llm_output,
            )
        # Parse out the action and action input
        regex = r"Action\s*\d*\s*:(.*?)\nAction\s*\d*\s*Input\s*\d*\s*:[\s]*(.*)"
        match = re.search(regex, llm_output, re.DOTALL)
        if not match:
            raise OutputParserException(f"Could not parse LLM output: `{llm_output}`")
        action = match.group(1).strip()
        action_input = match.group(2)
        # Return the action and action input
        return AgentAction(tool=action, tool_input=action_input.strip(" ").strip('"'), log=llm_output)
    
def authenticated_only(f):
    @wraps(f)
    def wrapped(*args, **kwargs):
        if not current_user.is_authenticated:
            disconnect()
        else:
            return f(*args, **kwargs)
    return wrapped


@socketio.on('connect')
def handle_connect():
    emit('message', {'text': 'Connected'})

@socketio.on('disconnect')
def disconnect():
    uuid = sanitize_filename(request.headers.get('uuid'))
    # Try to remove the tree; if it fails, throw an error using try...except.
    for folder in ["temp/","images/","output/", "vectors/"]:
        dir = folder + uuid + "/"
        try:
            shutil.rmtree(dir)
            print(f"INFO: folder {dir} delete suscesfully", flush=True)
        except OSError as e:
            #pass
            print("Error: %s - %s." % (e.filename, e.strerror), flush=True)

@login_manager.user_loader
def load_user(user_id):
    return User(user_id)


@app.route('/login', methods=['POST'])
def login():
    if request.cookies.get('session'):
        return jsonify({'status': 200}), 200

    session['foo'] = 'barbar'
    uuid = request.json.get('uuid')
    user = User(uuid)
    login_user(user, remember=True, force=True)
    return jsonify({'status': 200, 'userId': uuid}), 200



@app.route('/send-message', methods=['GET', 'POST'])
@login_required
def send_message():
    message = request.json.get('message', '')
    history = request.json.get('history', '')
    userId = sanitize_filename(request.json.get('userId', ''))
    print('INFO: Message recived ', flush=True)

     # ************ HEALTH DATA VECTOR ************
    llm = OpenAI(temperature=0)

    embeddings = OpenAIEmbeddings()
    memory = ConversationBufferMemory(memory_key="chat_history")
    readonlymemory = ReadOnlySharedMemory(memory=memory)

    vectorstore = Chroma(persist_directory=f"./vectors/{userId}/", embedding_function=embeddings)

    PROMPT = PromptTemplate(
        template=prompt_template_health_vector, input_variables=["context", "question"]
    )
    chain_type_kwargs = {"prompt": PROMPT}

    health_data_vectorstore = RetrievalQA.from_chain_type(
        llm=llm, 
        chain_type="stuff", 
        retriever=vectorstore.as_retriever(), 
        chain_type_kwargs=chain_type_kwargs,
        memory=readonlymemory,
    )

    
    # ************ PUBMED RETRIEVER ************
    pubmed_chain = RetrievalQA.from_chain_type(
        llm=llm, 
        chain_type="stuff", 
        retriever=PubMedRetriever(top_k_results=5),
        memory=readonlymemory,
        return_source_documents=True, 
        input_key="question"
    )

    # ************ MEDICAL TRIALS VECTOR ************
    instructor_embeddings = HuggingFaceInstructEmbeddings(model_name = 'hkunlp/instructor-base')

    vectorstore_trials = Chroma(persist_directory="breast-cancer-S1500-O200-keys/", embedding_function = instructor_embeddings)

    PROMPT = PromptTemplate(
        template=prompt_template_trials_vector, input_variables=["context", "question"]
    )
    chain_type_kwargs = {"prompt": PROMPT}

    clinical_trials_vectorstore = RetrievalQA.from_chain_type(
        llm=ChatOpenAI(model_name="gpt-4",temperature=0.3), 
        chain_type="stuff", 
        retriever=vectorstore_trials.as_retriever(),
        chain_type_kwargs=chain_type_kwargs,
        return_source_documents=True, input_key="question"
    )

    # ************ TOOLS ************
    tools = [
        Tool(
            name = "pubmed-query-search",
            func = lambda query: pubmed_chain({"question": query}),
            description = "Pubmed Query Search - Useful to obtain health information resources for recommendations from pubmed, Input should be a fully formed question."
        ),
        Tool(
            name="health-documents-vector",
            func=health_data_vectorstore.run,
            description="Health Documents QA - useful to obtain information about the users private health data. The input should be the original question",

        ),
        Tool(
            name="clinical-trials-vector",
            func=lambda query: clinical_trials_vectorstore({"question": query}),
            description="Clinical Trials Match - useful to to find a Clinical Trial with the users data (inclusion and exclusion criteria). The input should be the inclusion and exclusion criteria and related data",

        )
    ]

    # ************ CUSTOM AGENT ************
    prompt_with_history = CustomPromptTemplate(
        template=template,
        tools=tools,
        # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically
        # This includes the `intermediate_steps` variable because that is needed
        input_variables=["input", "intermediate_steps", "chat_history"]
    )

    output_parser = CustomOutputParser()
    callback_fn = MyCallbackHandler(q)
    # LLM chain consisting of the LLM and a prompt
    llm_chain = LLMChain(llm=ChatOpenAI(streaming=True, callbacks=[callback_fn],model_name='gpt-4', temperature=0), prompt=prompt_with_history)

    tool_names = [tool.name for tool in tools]
    agent = LLMSingleActionAgent(
        llm_chain=llm_chain,
        output_parser=output_parser,
        stop=["\nObservation:"],
        allowed_tools=tool_names
    )

    agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, memory=memory)
    def generate(rq: Queue):

      while(True):
        result = rq.get()
        if result == " END":
            break
        if result is not None:
            yield result
        else:
            break

    agent_thread = threading.Thread(target=agent_executor.run, args=(message,))
    agent_thread.start()
    return Response(stream_with_context(generate(q)), content_type='text/event-stream')

class MyCallbackHandler(BaseCallbackHandler):
    def __init__(self, q):
        self.q = q
        self.last_tokens = []
        self.answer_reached = False

    def on_llm_start(self, *args, **kwargs) -> None:
        """Run when LLM starts running."""
        with q.mutex:
            q.queue.clear()
    def on_llm_new_token(self, token, **kwargs) -> None:
        self.last_tokens.append(token)

        if self.answer_reached:
            self.q.put(token)

        if len(self.last_tokens) > 3:
            self.last_tokens.pop(0)

        if ''.join(self.last_tokens)=="Final Answer:":
            self.answer_reached = True

    def on_llm_end(self, *args, **kwargs) -> None:
        return self.q.empty()


        
@app.route('/upload', methods=['GET', 'POST'])
@login_required
def upload_files():
    delete_old_files = request.form.get('deleteOldFiles', 'true') == 'true'
    session['progress'] = 10
    session['message'] = 'Processing files'
    
    user_id = sanitize_filename(request.form.get('userId', ''))+"/"

    if not os.path.exists('temp/'): os.makedirs('temp/')
    if not os.path.exists('images/'): os.makedirs('images/')
    if not os.path.exists('output/'): os.makedirs('output/')

    # Crear una carpeta con el ID del usuario
    temp_path = 'temp/' + user_id
    images_path = 'images/' + user_id
    output_path = 'output/' + user_id
    if not os.path.exists(temp_path):
        os.makedirs(temp_path)
        os.makedirs(images_path)
        os.makedirs(output_path)

    
    # Check if files were sent
    if 'files' not in request.files:
        return 'No files uploaded', 400
    
    uploaded_files = request.files.getlist('files')

    # Get the names of the old files
    old_files = get_file_names(temp_path) + get_file_names(images_path) + get_file_names(output_path)
    
    # Process each uploaded file
    for file in uploaded_files:
        # Check if the file is already present
        if file.filename in old_files:
            print(f'INFO: Skipping file {file.filename}, already processed')
            continue
        
        
        # Save the file or perform any desired operations
        filename = file.filename.split(".")
        if filename[1] =='pdf':
            file.save(temp_path + file.filename)

        else:
            file.save(output_path+file.filename)
            
    print('INFO: PyPDF Process')
    session['progress'] = 33
    session['message'] = 'Extracting key information'
    pypdf_process(old_files, images_path, output_path, temp_path)
    
    session['progress'] = 66
    session['message'] = 'Training model'
    print('INFO: Create Vector', flush=True)
    create_vector(output_path)

    print('INFO: Create Vector Successfully: ' + user_id, flush=True)
    session['progress'] = 100
    session['message'] = 'All done!'
    
    return 'Files uploaded successfully'


@app.route('/progress')
def progress():
    return jsonify({'progress': session.get('progress', 0), 'message': session.get('message', '')})


def get_file_names(dir):
    if os.path.exists(dir):
        return [f for f in os.listdir(dir) if os.path.isfile(os.path.join(dir, f))]
    return []

def pypdf_process(old_files, images_path, output_path, temp_path):
    files = os.listdir(temp_path)

    for file in files:
        if file in old_files:
            print(f'INFO: Skipping image {file}, already processed')
            continue
        reader = PdfReader(temp_path+file)
        pages = len(reader.pages)

        text_list = []
        count = 0
        images_text = []

        #Get images and text for every page
        for page_num in range(pages):
            page = reader.pages[page_num]
            text_list.append(page.extract_text())

            #Get images in page
            for image_file_object in page.images:
                with open(images_path + str(page_num) + "_" + file.split(".")[0] + "_" + str(count) + "_" + image_file_object.name, "wb") as fp:
                    fp.write(image_file_object.data)
                images_text.append(pytesseract.image_to_string(Image.open(images_path + str(page_num) + "_" + file.split(".")[0] + "_" + str(count) + "_" + image_file_object.name)))
                count += 1

        #Save text in txt
        if images_text:
            with open(f'{output_path}image_{file.split(".")[0]}.txt', 'w') as f:
                for line in images_text:
                    clean_line = line.replace("\n\n","\n")
                    f.write(f'{clean_line}\n\n')

        #Save text from pdf
        with open(f'{output_path}{file.split(".")[0]}.txt', 'w') as f:
            for line in text_list:
                f.write(f"{line}\n")

def create_vector(output_path):
    # ************ HEALTH DATA VECTOR ************
    if not os.path.exists('vectors/'): os.makedirs('vectors/')
    llm = OpenAI(temperature=0)

    loader = DirectoryLoader(output_path)
    documents = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    documents = text_splitter.split_documents(documents)
    embeddings = OpenAIEmbeddings()

    vector_folder = "./vectors/"+output_path[:-1].split("/")[1]

    vectorstore = Chroma.from_documents(documents, embeddings, persist_directory=vector_folder)
    vectorstore.persist()
    
if __name__ == '__main__':
    socketio.run(app)
